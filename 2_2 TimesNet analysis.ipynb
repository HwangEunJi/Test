{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df1 = pd.read_csv(\"https://raw.githubusercontent.com/ourownstory/neuralprophet-data/main/datasets_raw/wp_log_peyton_manning.csv\")\n",
    "# df1.to_csv('c:/Users/Sofia/Documents/paper/Test/dataset/wp_log_peyton_manning.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available gpu: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(f\"available gpu: {torch.cuda.is_available()}\")\n",
    "\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, 'c:/Users/Sofia/Documents/paper/Test/TimesNet_code')\n",
    "from print_args import print_args\n",
    "from exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "\n",
    "fix_seed = 2024\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           pm_ver1             Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          c:/Users/Sofia/Documents/paper/Test/dataset/\n",
      "  Data Path:          wp_log_peyton_manning.csvFeatures:           S                   \n",
      "  Target:             y                   Freq:               d                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "  Output Attention:   0                   \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      1e-06               \n",
      "  Des:                test                Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            0                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0                   \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_pm_ver1_TimesNet_custom_ftS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m\n\u001b[0;32m     92\u001b[0m setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ll\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dm\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nh\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_el\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_fc\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dt\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     93\u001b[0m     args\u001b[38;5;241m.\u001b[39mtask_name,\n\u001b[0;32m     94\u001b[0m     args\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     args\u001b[38;5;241m.\u001b[39mdistil,\n\u001b[0;32m    109\u001b[0m     args\u001b[38;5;241m.\u001b[39mdes, ii)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m--> 112\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[0;32m    115\u001b[0m exp\u001b[38;5;241m.\u001b[39mtest(setting)\n",
      "File \u001b[1;32mc:\\Users/Sofia/Documents/paper/Test/TimesNet_code\\exp_long_term_forecasting.py:84\u001b[0m, in \u001b[0;36mExp_Long_Term_Forecast.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, setting):\n\u001b[1;32m---> 84\u001b[0m     train_data, train_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     vali_data, vali_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m     test_data, test_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users/Sofia/Documents/paper/Test/TimesNet_code\\exp_long_term_forecasting.py:30\u001b[0m, in \u001b[0;36mExp_Long_Term_Forecast._get_data\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, flag):\n\u001b[1;32m---> 30\u001b[0m     data_set, data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_set, data_loader\n",
      "File \u001b[1;32mc:\\Users/Sofia/Documents/paper/Test/TimesNet_code\\data_factory.py:63\u001b[0m, in \u001b[0;36mdata_provider\u001b[1;34m(args, flag)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     62\u001b[0m     drop_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m data_set \u001b[38;5;241m=\u001b[39m \u001b[43mData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_test_split_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_test_split_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeenc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeenc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonal_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseasonal_patterns\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(flag, \u001b[38;5;28mlen\u001b[39m(data_set))\n\u001b[0;32m     78\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     79\u001b[0m     data_set,\n\u001b[0;32m     80\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     81\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle_flag,\n\u001b[0;32m     82\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m     83\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39mdrop_last)\n",
      "File \u001b[1;32mc:\\Users/Sofia/Documents/paper/Test/TimesNet_code\\data_loader.py:52\u001b[0m, in \u001b[0;36mDataset_Custom.__init__\u001b[1;34m(self, root_path, flag, size, features, data_path, date_col, train_test_split_rate, train_test_split_date, target, scale, timeenc, freq, seasonal_patterns)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_path \u001b[38;5;241m=\u001b[39m root_path\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m data_path\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__read_data__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users/Sofia/Documents/paper/Test/TimesNet_code\\data_loader.py:100\u001b[0m, in \u001b[0;36mDataset_Custom.__read_data__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m     data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     99\u001b[0m df_stamp \u001b[38;5;241m=\u001b[39m df_raw[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_col]][border1:border2]\n\u001b[1;32m--> 100\u001b[0m df_stamp[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_stamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeenc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    103\u001b[0m     df_stamp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_stamp[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: row\u001b[38;5;241m.\u001b[39mmonth, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sofia\\Documents\\paper\\code\\venv\\py39\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    # basic config\n",
    "    task_name = 'long_term_forecast'\n",
    "    , is_training = 1\n",
    "    , model_id = 'pm_ver1'\n",
    "    , model = 'TimesNet'\n",
    "\n",
    "    # data loader\n",
    "    , data = 'custom'\n",
    "    , root_path = 'c:/Users/Sofia/Documents/paper/Test/dataset/' ### 데이터 확인 필요\n",
    "    , data_path = 'wp_log_peyton_manning.csv' ### 데이터 확인 필요\n",
    "    , features = 'S' ### M: 다변량 -> 다변량, S: 일변량 -> 일변량, MS: 다변량 -> 일변량\n",
    "    , target = 'y'\n",
    "    , date_col = 'ds'\n",
    "    , train_test_split_date = ['2012-11-09', '2014-06-16']\n",
    "    , train_test_split_rate = None\n",
    "    , freq = 'd'\n",
    "    , checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    , seq_len = 96\n",
    "    , label_len = 48\n",
    "    , pred_len = 96\n",
    "    , seasonal_patterns = 'Monthly'\n",
    "    , inverse = False\n",
    "\n",
    "    # inputation task ### 삭제?\n",
    "    , mask_rate = 0.25 \n",
    "\n",
    "    # anomaly detection task ### 삭제?\n",
    "    , anomaly_ratio = 0.25 \n",
    "\n",
    "    # model define\n",
    "    , top_k = 5\n",
    "    , num_kernels = 6 # N: Transformer 주요 파라미터\n",
    "    , enc_in = 1 # 사용 변수 수\n",
    "    , dec_in = 1 # 사용 변수 수\n",
    "    , c_out = 1 # 사용 변수 수\n",
    "    , d_model = 512 # Transformer 주요 파라미터\n",
    "    , n_heads = 8 # h: Transformer 주요 파라미터\n",
    "    , e_layers = 2\n",
    "    , d_layers = 1\n",
    "    , d_ff = 2048 # Transformer 주요 파라미터\n",
    "    , moving_avg = 25\n",
    "    , factor = 1\n",
    "    , distil = True\n",
    "    , dropout = 0.1\n",
    "    , embed = 'timeF'\n",
    "    , activation = 'gelu'\n",
    "    , output_attention = False ### default 없음 추가로 arg 지정 없으면 false\n",
    "\n",
    "    # optimization\n",
    "    , num_workers = 10\n",
    "    , itr = 1\n",
    "    , train_epochs = 10\n",
    "    , batch_size = 32 # 32\n",
    "    , patience = 3 #\n",
    "    , learning_rate = 0.000001\n",
    "    , des = 'test'\n",
    "    , loss = 'MSE'\n",
    "    , lradj = 'type1'\n",
    "    , use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    , use_gpu = False\n",
    "    , gpu = 0\n",
    "    , use_multi_gpu = False\n",
    "    , devices = '0'\n",
    "\n",
    "    # de-stationary projector params\n",
    "    , p_hidden_dims = [128, 128]\n",
    "    , p_hidden_layers = 2\n",
    ")\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print_args(args)\n",
    "\n",
    "Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        exp = Exp(args)  # set experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # (1743, 581, 581)\n",
    "    # (1552, 486, 486)\n",
    "    # -191, -95, -95\n",
    "\n",
    "#PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           pm_ver2             Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          c:/Users/Sofia/Documents/paper/Test/dataset/\n",
      "  Data Path:          wp_log_peyton_manning.csvFeatures:           S                   \n",
      "  Target:             y                   Freq:               d                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              10                  Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "  Output Attention:   0                   \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       1                   Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      1e-06               \n",
      "  Des:                test                Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            0                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0                   \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_pm_ver2_TimesNet_custom_ftS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1552\n",
      "val 486\n",
      "test 486\n",
      "Epoch: 1 cost time: 193076.02822303772\n",
      "Epoch: 1, Steps: 48 | Train Loss: 1.3552519 Vali Loss: 1.7277331 Test Loss: 1.2094432\n",
      "Validation loss decreased (inf --> 1.727733).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      ">>>>>>>testing : long_term_forecast_pm_ver2_TimesNet_custom_ftS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 486\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "test shape: (486, 1, 96, 1) (486, 1, 96, 1)\n",
      "test shape: (486, 96, 1) (486, 96, 1)\n",
      "mse:1.209443211555481, mae:0.8663414716720581\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    # basic config\n",
    "    task_name = 'long_term_forecast'\n",
    "    , is_training = 1\n",
    "    , model_id = 'pm_ver2'\n",
    "    , model = 'TimesNet'\n",
    "\n",
    "    # data loader\n",
    "    , data = 'custom'\n",
    "    , root_path = 'c:/Users/Sofia/Documents/paper/Test/dataset/' ### 데이터 확인 필요\n",
    "    , data_path = 'wp_log_peyton_manning.csv' ### 데이터 확인 필요\n",
    "    , features = 'S' ### M: 다변량 -> 다변량, S: 일변량 -> 일변량, MS: 다변량 -> 일변량\n",
    "    , target = 'y'\n",
    "    , freq = 'd'\n",
    "    , checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    , seq_len = 96\n",
    "    , label_len = 48\n",
    "    , pred_len = 96\n",
    "    , seasonal_patterns = 'Monthly'\n",
    "    , inverse = False\n",
    "\n",
    "    # inputation task ### 삭제?\n",
    "    , mask_rate = 0.25 \n",
    "\n",
    "    # anomaly detection task ### 삭제?\n",
    "    , anomaly_ratio = 0.25 \n",
    "\n",
    "    # model define\n",
    "    , top_k = 10\n",
    "    , num_kernels = 6 # N: Transformer 주요 파라미터\n",
    "    , enc_in = 1 # 사용 변수 수\n",
    "    , dec_in = 1 # 사용 변수 수\n",
    "    , c_out = 1 # 사용 변수 수\n",
    "    , d_model = 512 # Transformer 주요 파라미터\n",
    "    , n_heads = 8 # h: Transformer 주요 파라미터\n",
    "    , e_layers = 2\n",
    "    , d_layers = 1\n",
    "    , d_ff = 2048 # Transformer 주요 파라미터\n",
    "    , moving_avg = 25\n",
    "    , factor = 1\n",
    "    , distil = True\n",
    "    , dropout = 0.1\n",
    "    , embed = 'timeF'\n",
    "    , activation = 'gelu'\n",
    "    , output_attention = False ### default 없음 추가로 arg 지정 없으면 false\n",
    "\n",
    "    # optimization\n",
    "    , num_workers = 10\n",
    "    , itr = 1\n",
    "    , train_epochs = 1\n",
    "    , batch_size = 32 # 32\n",
    "    , patience = 3 #\n",
    "    , learning_rate = 0.000001\n",
    "    , des = 'test'\n",
    "    , loss = 'MSE'\n",
    "    , lradj = 'type1'\n",
    "    , use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    , use_gpu = False\n",
    "    , gpu = 0\n",
    "    , use_multi_gpu = False\n",
    "    , devices = '0'\n",
    "\n",
    "    # de-stationary projector params\n",
    "    , p_hidden_dims = [128, 128]\n",
    "    , p_hidden_layers = 2\n",
    ")\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print_args(args)\n",
    "\n",
    "Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        exp = Exp(args)  # set experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # (1743, 581, 581)\n",
    "    # (1552, 486, 486)\n",
    "    # -191, -95, -95\n",
    "\n",
    "#PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           pm_ver3             Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          c:/Users/Sofia/Documents/paper/Test/dataset/\n",
      "  Data Path:          wp_log_peyton_manning.csvFeatures:           S                   \n",
      "  Target:             y                   Freq:               d                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              15                  Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "  Output Attention:   0                   \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       1                   Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      1e-06               \n",
      "  Des:                test                Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            0                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0                   \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_pm_ver3_TimesNet_custom_ftS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1552\n",
      "val 486\n",
      "test 486\n",
      "Epoch: 1 cost time: 322212.52312874794\n",
      "Epoch: 1, Steps: 48 | Train Loss: 1.2752148 Vali Loss: 1.7044386 Test Loss: 1.1630592\n",
      "Validation loss decreased (inf --> 1.704439).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      ">>>>>>>testing : long_term_forecast_pm_ver3_TimesNet_custom_ftS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 486\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "out5: 1\n",
      "test shape: (486, 1, 96, 1) (486, 1, 96, 1)\n",
      "test shape: (486, 96, 1) (486, 96, 1)\n",
      "mse:1.163059115409851, mae:0.8539007306098938\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    # basic config\n",
    "    task_name = 'long_term_forecast'\n",
    "    , is_training = 1\n",
    "    , model_id = 'pm_ver3'\n",
    "    , model = 'TimesNet'\n",
    "\n",
    "    # data loader\n",
    "    , data = 'custom'\n",
    "    , root_path = 'c:/Users/Sofia/Documents/paper/Test/dataset/' ### 데이터 확인 필요\n",
    "    , data_path = 'wp_log_peyton_manning.csv' ### 데이터 확인 필요\n",
    "    , features = 'S' ### M: 다변량 -> 다변량, S: 일변량 -> 일변량, MS: 다변량 -> 일변량\n",
    "    , target = 'y'\n",
    "    , freq = 'd'\n",
    "    , checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    , seq_len = 96\n",
    "    , label_len = 48\n",
    "    , pred_len = 96\n",
    "    , seasonal_patterns = 'Monthly'\n",
    "    , inverse = False\n",
    "\n",
    "    # inputation task ### 삭제?\n",
    "    , mask_rate = 0.25 \n",
    "\n",
    "    # anomaly detection task ### 삭제?\n",
    "    , anomaly_ratio = 0.25 \n",
    "\n",
    "    # model define\n",
    "    , top_k = 15\n",
    "    , num_kernels = 6 # N: Transformer 주요 파라미터\n",
    "    , enc_in = 1 # 사용 변수 수\n",
    "    , dec_in = 1 # 사용 변수 수\n",
    "    , c_out = 1 # 사용 변수 수\n",
    "    , d_model = 512 # Transformer 주요 파라미터\n",
    "    , n_heads = 8 # h: Transformer 주요 파라미터\n",
    "    , e_layers = 2\n",
    "    , d_layers = 1\n",
    "    , d_ff = 2048 # Transformer 주요 파라미터\n",
    "    , moving_avg = 25\n",
    "    , factor = 1\n",
    "    , distil = True\n",
    "    , dropout = 0.1\n",
    "    , embed = 'timeF'\n",
    "    , activation = 'gelu'\n",
    "    , output_attention = False ### default 없음 추가로 arg 지정 없으면 false\n",
    "\n",
    "    # optimization\n",
    "    , num_workers = 10\n",
    "    , itr = 1\n",
    "    , train_epochs = 1\n",
    "    , batch_size = 32 # 32\n",
    "    , patience = 3 #\n",
    "    , learning_rate = 0.000001\n",
    "    , des = 'test'\n",
    "    , loss = 'MSE'\n",
    "    , lradj = 'type1'\n",
    "    , use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    , use_gpu = False\n",
    "    , gpu = 0\n",
    "    , use_multi_gpu = False\n",
    "    , devices = '0'\n",
    "\n",
    "    # de-stationary projector params\n",
    "    , p_hidden_dims = [128, 128]\n",
    "    , p_hidden_layers = 2\n",
    ")\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print_args(args)\n",
    "\n",
    "Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        exp = Exp(args)  # set experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # (1743, 581, 581)\n",
    "    # (1552, 486, 486)\n",
    "    # -191, -95, -95\n",
    "\n",
    "#PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
